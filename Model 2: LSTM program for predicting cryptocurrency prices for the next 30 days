# Model 2: Fine-tuning the hyperparameters can significantly improve the performance of the LSTM model. Here, we'll focus on adjusting the learning rate, batch size, number of epochs, and the number of units in the LSTM layers. We will also introduce dropout layers to prevent overfitting.
Here's an improved version of the code with fine-tuned hyperparameters:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Load data
df = pd.read_csv('cryptocurrency_price_data.csv')  # replace with your dataset file

# Preprocess data
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)
data = df['Close'].values.reshape(-1, 1)

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# Prepare the dataset for the LSTM
def create_dataset(dataset, look_back=60):
    X, Y = [], []
    for i in range(len(dataset) - look_back - 1):
        X.append(dataset[i:(i + look_back), 0])
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)

look_back = 60
X, Y = create_dataset(scaled_data, look_back)
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# Split the data into training and testing sets
train_size = int(len(X) * 0.8)
test_size = len(X) - train_size
X_train, X_test = X[:train_size], X[train_size:]
Y_train, Y_test = Y[:train_size], Y[train_size:]

# Build the LSTM model with fine-tuned hyperparameters
model = Sequential()
model.add(LSTM(100, return_sequences=True, input_shape=(look_back, 1)))
model.add(Dropout(0.2))
model.add(LSTM(100, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(50))
model.add(Dense(1))

# Compile the model with a lower learning rate
model.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')

# Train the model with a larger batch size and more epochs
model.fit(X_train, Y_train, batch_size=32, epochs=50)

# Make predictions
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

# Invert predictions
train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)

# Plot the results
train = data[:train_size + look_back]
valid = data[train_size + look_back:]
valid['Predictions'] = test_predict

plt.figure(figsize=(16, 8))
plt.title('Cryptocurrency Price Prediction')
plt.xlabel('Date')
plt.ylabel('Price')
plt.plot(train)
plt.plot(valid[['Close', 'Predictions']])
plt.legend(['Train', 'Valid', 'Predictions'], loc='lower right')
plt.show()

# Predict the next 30 days
last_60_days = scaled_data[-look_back:]
next_30_days = []

for _ in range(30):
    last_60_days_input = np.reshape(last_60_days, (1, look_back, 1))
    next_day_pred = model.predict(last_60_days_input)
    next_30_days.append(next_day_pred[0, 0])
    last_60_days = np.append(last_60_days[1:], next_day_pred, axis=0)

next_30_days = scaler.inverse_transform(np.array(next_30_days).reshape(-1, 1))

# Plot the next 30 days predictions
plt.figure(figsize=(16, 8))
plt.title('Cryptocurrency Price Prediction for Next 30 Days')
plt.xlabel('Days')
plt.ylabel('Price')
plt.plot(next_30_days)
plt.show()

# Improvements:
1.	LSTM Units: Increased the number of units in the LSTM layers to 100 for better learning capacity.
2.	Dropout Layers: Added Dropout layers with a rate of 0.2 to prevent overfitting.
3.	Learning Rate: Reduced the learning rate to 0.0005 for more gradual and stable learning.
4.	Batch Size: Increased the batch size to 32 for more efficient training.
5.	Epochs: Increased the number of epochs to 50 to allow the model more time to learn from the data.
These changes should help improve the model's accuracy and generalization to new data. You can further experiment with these and other hyperparameters to find the optimal configuration for your specific dataset.
